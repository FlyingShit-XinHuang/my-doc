# Redis集群

Redis集群提供了一种将数据自动在多个节点中分片的方式。同时，Redis集群还提供了在网络分区期间一定程度的可用性，即一些节点故障或无法与之通信时集群可以继续提供服务。但当集群中出现较多故障节点时，整个集群将停止服务。

综上，Redis集群将提供以下特性：

* 将数据自动分片，且存储在不同节点上
* 当集群中一些节点故障时，仍可以保证可用性。

## 集群TCP端口

每个节点需要2个TCP连接，一个是用于服务Redis client（如6379）的指令端口，另一个端口号会增加10000（如16379）。第二个端口作为集群总线存在（Cluster bus），是一条基于二进制协议的节点间通信管道。集群总线用于故障检测、配置更新、故障转移授权等等。请确保在防火墙中打开了两个端口，否则Redis集群中的节点无法互相通信。指令端口和集群总线端口的偏移量固定为10000。

为保证Redis集群正常工作，对于每个节点：
* 指令端口需要对所有Redis clients以及集群中其他节点（用于主机迁移）开启。
* 集群总线端口只需保证集群中的节点间可以互相连通

集群总线使用的二进制协议与Redis客户端和服务端的通信协议不同，这种协议需要的带宽和处理时间很少，更适合节点间交换数据。

## 数据分片

Redis集群不使用一致性哈希，而是使用称之为哈希槽的概念，每个主键属于一个哈希槽。集群中共有16384个哈希槽，每个主键经过CRC16算法计算再对16384求模。
集群中每个节点负责一部分哈希槽，如果集群有3个节点，那么哈希槽分布可能是这样：

* A节点包含0到5500
* B节点包含5501到11000
* C节点包括11001到16384

这样就可以简便地在集群中添加或删除节点。如要添加新节点D，只需将一些哈希槽从A、B、C移到D。如果要删除节点A，原理类似，只需将A节点的哈希槽分到B和C，当节点A为空后就可以被删除。
移动哈希槽无需停止运行，所有添加删除节点、改变节点的哈希槽持有量都不需要停机。
Redis集群支持多个主键同时操作，只要指令中包括的这些主键属于同一个哈希槽。用户可以使用一个“哈希标签”的概念使得多个主键属于同一个哈希槽。
哈希标签主要是使用花括号（{}）包含一个子串，只有花括号中的子串会被计算哈希值。如this{foo}key和another{foo}key会被分配给同一个哈希槽。

## Redis集群一致性保证

Redis集群不能保证强一直性。这实际上意味着在一些特定条件下，Redis集群可能丢失更新操作结果，而这些操作已经告知客户端更新成功。

第一个原因是Redis使用了异步复制机制，在更新时会出现以下流程：

* 客户端向主节点B发送更新指令
* B回复OK
* B向从节点B1、B2、B3发送更新指令

B不会等待从节点的确认便会回复客户端，如果不这样会出现过高延迟。因此客户端执行了一些更新操作，B回复确认，但在向从节点发送更新指令前出现了故障。其中一个的从节点会被选为新的主节点，此时更新操作就永远丢失了。
Redis集群支持同步更新操作，是通过WAIT指令实现的，这种机制使得更新操作更难于丢失。但即使使用同步更新，Redis集群也不支持强一致性。在更复杂的场景下，集群有可能选择了一个从节点替换主节点，但这个从节点却恰巧没有收到更新操作。

使Redis集群丢失更新的另一个场景会出现在网络分区时，当一个客户端与少数节点（至少包含一个主节点）被隔离成一个分区时会出现更新丢失。
假设集群中有A、B、C、A1、B1、C1节点，三主三从。Z1是一个客户端。网络分区发生后，B和Z1可能会分在一个区，其他节点分在另一个区，此时B可以继续接收Z1的更新指令。如果网络分区持续了足够的时间，期间B1被选为了主节点。那么Z1向B发送的更新操作就会丢失。
Z1向B发送的更新指令数量是存在一个最大窗口的。如果网络分区持续的时间足够推选出一个新的主节点，那么少数节点所在的分区中所有的主节点会停止接收写操作。这段持续时间是一个被称为“节点超时”的重要配置。
节点超时后，一个主节点会被认为出现了故障，其副本中的一个从节点会成为新的主节点。类似地，一个主节点在节点超时后没能与大多数其他主节点恢复通信，该节点将进入一个错误状态并停止接受指令。

## 集群特性和设计原理

### Redis集群协议中客户端和服务端的角色

Redis集群的各节点负责存储数据、获取集群状态，还包括映射主键到正确节点。节点还能够自动发现其他节点、检测无法运行的节点、以及选举从节点变成主节点。
集群使用被称为Redis集群总线的二进制、TCP协议来连接各节点。节点间使用gossip协议来传输集群数据，以便发现新节点、发送ping包、发送消息来标识特殊条件。集群总线还用来在集群中传播Pub/Sub消息，以及策划用户请求的人工故障恢复。
由于节点无法代理请求，客户端可能收到重定向错误（-MOVED和-ASK）从而被重定向到其他节点。客户端可以不存储集群的状态，但能够缓存主键和节点映射的客户端可以提高访问性能。

### Redis集群主从模型

一部分主节点故障或者无法与其他大部分节点通信时，Redis集群使用主从模型保证可用性，每个哈希槽在模型中都有N个副本，1个在主节点中，N-1个在从节点中。
如果集群由A、B、C三个主节点组成，那么需要增加A1、B1、C1三个从节点，每个主节点对应一个从节点。B1会复制B节点，如果B出现故障，集群会选择B1作为新的主节点，使得系统继续运行。如果B和B1同时出现故障，集群将无法继续运行。

### 写安全

Redis集群的合并方法使用“最后一个故障转移生效”策略，就是说最后一个被选的主节点数据最终会替换所有副本。在网络分区发生期间总会存在一个丢失写操作的时间窗口，然而对于一个连接到多数主节点所在分区的客户端和一个连接点少数主节点所在分区的客户端来说，时间窗口是不同的。
Redis集群会尽更多努力来保留那些连接到多数主节点的客户端的写操作。以下是多数主节点所在分区接收的已被确认的写操作丢失的场景：

1. 如果主节点宕机而写操作还没有传播到从节点，且如果超过足够长的时间该主节点仍无法访问，它的从节点会被选为主节点，这时旧master的写操作将永远丢失。在一个突发的、彻底的主节点故障发生时，以上情况是很难看到的。因为主节点会尽力向客户端回复写确认的同时向从节点传递写操作。然而这种情况是现实中存在的故障方式。
2. 另一种理论上的丢失写操作的故障方式如下：

* 主节点由于网络分区而无法访问
* 该节点发生故障转移，被其中一个从节点替换
* 一段时间后该节点又可以被访问
* 在被转换成从节点之前，一个使用过期映射表的客户端可能会向该节点写数据。

第二种故障方式是很难发生的。因为在足够的时间内无法与其他多数主节点通信的主节点将被故障转移，该节点也将不再接收写操作。并且当网络分区修复时，写操作在短时间内依然会被拒绝，为的是让其他节点通知配置变更。

向少数主节点所在分区的主节点发送的写操作会存在更大的丢失窗口。例如网络分区是由少数主节点和至少一个客户端组成，如果这些主节点在大多数节点所在分区被故障转移，那么向这些主节点发送的写操作将会丢失。这种情况下Redis集群可能会丢失十分重要的一组更新操作。
将被故障转移的主节点无法被多数主节点访问的时间必须超过NODE\_TIMEOUT。如果网络分区在这段时间修复，不会有写操作丢失。如果网络分区持续时间超过NODE\_TIMEOUT，这段时间内的所有向少数主节点所在分区发送的写请求可能会丢失。但是当这些主节点无法与多数主节点通信时间超过NODE_TIMEOUT时，它们会开始拒绝写操作。因此当少数节点不可用时存在一个最大的丢失时间窗口，过了时间窗口后写操作会被拒绝。

### 可用性

在少数节点所在的网络分区中，Redis集群将不可用。假设在多数节点所在分区中，至少有大多数master以及每个无法访问的master对应的一个从节点，此时经过NODE_TIMEOUT时间和用于故障转移的额外几秒钟后（通常约1、2秒），集群会恢复可用性。Redis集群设计为在一些节点故障时可用，但不适用于要求在大量网络割裂事件发生时仍可用的应用。

副本迁移特性使得Redis集群的可用性在实际中得到了提升。故障发生时，当有主节点成为“孤儿”，集群会重新配置从节点结构，将副本迁移给“孤儿”。

### 性能

Redis集群的节点不会将指令代理到正确节点，而是将客户端重定向到正确节点。最终客户端会得到最新的主键和节点的映射关系，所以在正常操作期间，客户端都是直接访问正确的节点。
由于使用的异步复制机制，主节点不用等待从节点的确认操作。又因为多主键指令是受限的，故数据不会在节点间移动，重新分片除外。
准确的说，访问集群的正常操作是与访问单个Redis实例的处理是一样的。由于客户端与节点间使用长连接，因此延迟数据也与访问单独Redis实例是相同的。
保留微弱而合理的数据安全以及可用性形态的同时，又提供高性能和扩展性是Redis集群的主要目标。

## 组件简介

### 主键分布模型

主键空间被拆分到16384个槽中，也就设置了主节点数量的上限为16384，但是建议的节点数量最大值约为1000个。映射主键和哈希槽的基本算法为：HASH_SLOT= CRC16(key) mod 16384。Redis集群使用了CRC16的16位结果中的14位（16384=2的14次方）。CRC16将不同类型主键均分到16384个槽的实验过程中表现非常出色。

### 哈希标签

使用哈希标签的主键在计算哈希槽的时候有所不同。哈希标签是将不同主键映射到同一哈希槽的方法，从而实现集群中的多主键操作。哈希标签由花括号标记出来，由于可能在主键中存在多个花括号，为区分这种情况哈希标签的识别算法定义为：

* 如果主键包含“{”字符
* 并且之后存在“}”字符
* 并且在第一个“{”和第一个“}”之间存在至少一个字符

满足上述条件时，第一个“{”和第一个“}”之间的字符串会用来计算哈希值。下面是一些实例：

* {user1000}.following和{user1000}.followers会映射到同一哈希槽
* 对于主键foo{}{bar}，会用整个主键计算哈希值，因为第一个花括号内不存在字符串
* 对于foo{{bar}}zap，会用{bar计算哈希值，因为它是第一个“{”和第一个“}”之间的字符串
* 对于foo{bar}{zap}，会用bar计算哈希值
* 如果主键以{}开头，那么整个主键会用来计算哈希值。当使用二进制内容作为主键时，可以使用此方法。

### 节点属性

每个节点都有一个ID，是用16进制表示的160位随机数，这个随机数是在节点首次启动是生成的（通常使用/dev/urandom）。ID将会保存在节点的配置文件（cluster-config-file指定的文件）中。除非删除节点配置文件或使用CLUSTER RESET指令，否则节点ID将永久生效。
每个节点都会维护的其他节点信息有：节点ID、该节点的IP和端口、一些标记、主节点ID（如果该节点为从节点）、最后ping通的时间和最后接收pong的时间、该节点当前的配置纪元、连接状态和哈希槽集合。

### 集群拓扑结构

Redis集群是一个完整的多边形网格，每个节点都使用TCP连接到其他节点。在N个节点的集群中，每个节点都有N-1个向其他节点主动建立的TCP连接和N-1个其他节点主动建立的TCP连接。这些TCP连接一直保持着长连接，而不是按需创建。
节点间使用了gossip协议和一个配置更新机制，从而避免了交换大量的消息。消息交换的数量不是指数级增长的。

### 节点握手

节点总会用集群总线端口监听连接请求，并回复ping即使ping是不受信任的。但是如果发送节点不是集群的一部分，其发送的网络包会被接收节点丢弃。节点只会通过以下两种方法确认其他节点是集群的一部分：

* 一个节点使用MEET消息表名身份。一个MEET消息与PING一样，但会强制接收者确认发送节点在集群中。只有使用CLUSTER MEET ip port指令时，节点才会向其他节点发送MEET消息。
* 如果A知道B，B知道C，最终B会向A发送关于C的消息。此时A将C注册为集群的一部分，并尝试与C建立连接。

只要在拓扑结构中加入了一些节点，最终这些节点会自动与其他节点建立连接。也就意味着集群能够自动发现其他节点，但需要强制创建一个信任关系。

## 重定向和重新分片

### MOVED重定向

Redis客户端可以向任意的节点（包括从节点）发送请求。接收请求的节点会分析请求，如果是单主键或有相同哈希标签的多主键操作，节点会查找主键对应的哈希槽以及所在节点。如果哈希槽不属于该节点，那么会回复客户端一个MOVED错误，例如：

<pre>
  -MOVED 3999 127.0.0.1:6381
</pre>

错误中包含了主键对应的哈希槽（3999）和哈希槽所在的实例的ip和端口。客户端需要重新发送请求到指定的实例。
客户端必须也能处理-ASK重定向。

### 在线重配置

哈希槽移动时会用到以下指令：

* CLUSTER ADDSLOTS slot1 [slot2] ... [slotN]
* CLUSTER DELSLOTS slot1 [slot2] ... [slotN]
* CLUSTER SETSLOT slot NODE node
* CLUSTER SETSLOT slot MIGRATING node
* CLUSTER SETSLOT slot IMPORTING node

前两个指令就是简单地用于向节点分配或在节点中删除哈希槽。分配后指定的哈希槽在集群中转移到当前主节点。ADDSLOTS通常用于新集群创建伊始向各主节点分配哈希槽。
第三条指令用来将哈希槽分配给指定节点。第四条和第五条指令用来迁移哈希槽：

* 只要主键还存在，处理第四条指令的节点会接收所有关于正在迁出的哈希槽的请求。否则将返回-ASK重定向错误。
* 只要请求跟在ASKING指令后面，处理第五条指令的节点将会处理所有关于正在导入的哈希槽的请求。如果客户端没有发送ASKING指令，节点将返回-MOVED重定向错误。

假设A和B是两个主节点，8号哈希槽将从A移到B，我们将发送以下指令：

* 向B发送：CLUSTER SETSLOT 8 IMPORTING A
* 向A发送：CLUSTER SETSLOT 8 MIGRATING B

其他所有节点会继续将访问8号哈希槽的请求指向A节点，此时会发生：

* 所有对还存在主键的请求将用A处理
* 所有不存在的主键将由B处理，因为A会将客户端重定向到B。

这样A将不再有新的主键产生，再通过以下指令将已有主键将从A移到B：

<pre>
  CLUSTER GETKEYSINSLOT slot count
</pre>

该指令会返回指定节点指定个数的主键。再向A节点发送MIGRATE指令将每个主键迁移到B节点。执行该指令时，为了避免竞争条件，两个节点会短暂的锁住。MIGRATE指令如下：

<pre>
  MIGRATE target_host target_port key target_database timeout
</pre>

MIGRATE指令将连接到目标实例，发送序列化的主键数据，当接收到OK后，该主键将被删除。从客户端的角度看，在任意指定的时间点，该主键或是在A上或是在B上。
通常来说迁移时数据库只需指定为0。
当迁移过程最终结束，SETSLOT slot NODE id指令将发送到两个与迁移相关的节点，以便将哈希槽再次置为正常状态。该指令通常会发送到其他节点，以避免在节点中传递新的配置信息。

### ASK重定向

为什么不只用MOVED，而还需要ASK？MOVED意思是哈希槽是永久存在于其他节点，后续所有请求应该访问指定节点。ASK意思是只有下一条请求需要访问指定节点。对于一个设置为IMPORTING的哈希槽，只有客户端在ASKING指令之后发送的查询，B节点才会接收。

ASKING指令为客户端设置了一次性的标记，使得节点去处理一个与IMPORTING哈希槽相关的查询。
从客户端角度来说，一个完整的ASK重定向语义为：

* 如果客户端收到ASK重定向，只向指定节点发送被重定向的那一条指令，后续指令继续发送的原目标。
* 重定向请求以ASKING指令开始
* 暂不更新本地的映射关系

一旦8号哈希槽迁移完成，A会向访问8号槽的客户端返回MOVED错误，客户端便可以永久更新本地映射关系了。如果客户端提前更新了映射关系也不会出错，由于没有在查询前发生ASKING指令，B节点会返回MOVED错误，将请求重定向到A。

### 客户端初次连接和处理重定向

客户端可能不会实现哈希槽和节点映射关系的缓存，只是随机的访问并触发重定向。这样的实现是非常低效的。Redis客户端应该缓存哈希槽的配置，配置不要求是时新的。只要保证重定向时更新即可。

客户端可能在收到重定向后更新访问槽映射关系，这样也是低效的。客户端应该更新整个映射表，因为哈希槽迁移通常都是批量的。客户端程序可以适应CLUSTER NODES指令获取节点信息并解析最新的映射关系，但在3.0版本后可以使用CLUSTER SLOTS指令来直接获取映射关系。使用示例为：

<pre>
127.0.0.1:30001> cluster slots

1) 1) (integer) 1000
   2) (integer) 5961
   3) 1) "127.0.0.1"
      2) (integer) 30001
   4) 1) "127.0.0.1"
      2) (integer) 30004
2) 1) (integer) 10923
   2) (integer) 11421
   3) 1) "127.0.0.1"
      2) (integer) 30001
   4) 1) "127.0.0.1"
      2) (integer) 30004
3) 1) (integer) 500
   2) (integer) 999
   3) 1) "127.0.0.1"
      2) (integer) 30005
   4) 1) "127.0.0.1"
      2) (integer) 30002
4) 1) (integer) 5962
   2) (integer) 10922
   3) 1) "127.0.0.1"
      2) (integer) 30005
   4) 1) "127.0.0.1"
      2) (integer) 30002
5) 1) (integer) 0
   2) (integer) 499
   3) 1) "127.0.0.1"
      2) (integer) 30003
   4) 1) "127.0.0.1"
      2) (integer) 30006
6) 1) (integer) 11422
   2) (integer) 16383
   3) 1) "127.0.0.1"
      2) (integer) 30003
   4) 1) "127.0.0.1"
      2) (integer) 30006
</pre>

返回结果为数组类型，每个元素记录了哈希槽范围和所属的主从节点。每个元素的子元素也是数组类型，前两个子元素为哈希槽的开始和结束编号，第三、四个子元素为主节点ip和端口，其余子元素为正常状态的从节点ip和端口。
CLUSTER SLOTS指令不能保证返回的结果会覆盖16384个哈希槽。所以客户端需要向访问这些哈希槽的操作返回错误。并且在返回错误之前应该再次更新映射关系，以检查未覆盖的哈希槽是否已经分配。

### 多主键操作

在集群中的多主键操作需要使用相同的哈希标签。在哈希槽迁移过程中，如果主键都存在于相同节点，那么多主键操作是可用的。但如果存在于不同节点，会产生-TRYAGAIN错误。客户端可以隔段时间进行重试，或直接返回错误。

### 使用从节点实现读扩展

正常情况下，从节点会将请求重定向到哈希槽所属的主节点，但使用READONLY指令可以使从节点处理读请求从而实现读扩展。使用READWRITE指令可以关闭从节点的只读属性。

## 容错处理

### 心跳和gossip消息

集群中的各节点会持续交换ping和pong包，这两种包有相同的结构，都包含了重要的配置信息，只是类型域的值是不同的。我们统称两种包为心跳包。通常pong包会由ping包的接收触发，但节点也有可能会向其他节点主动发送pong包以更新配置。节点每秒都会随机选择一些节点来发送心跳包，这样每个节点发送心跳包的总量不会受节点总数的影响。但如果在超过NODE_TIMEOUT一半的时间后，还没有向一些节点发送ping包或接收这些节点的pong包，当前节点会保证向这些节点发送ping包。

集群中交换消息的数量会受到NODE\_TIMEOUT设置和节点数量影响。例如集群中有100个节点，NODE\_TIMEOUT设置为60秒。每个节点会在30秒内尝试发送99个ping包，平均每秒3.3个包。100个节点每秒平均发送330个ping包。

### 心跳包结构

ping和pong包的头包含以下信息：

* 节点ID，160位
* 发送节点的currentEpoch和configEpoch域，这两个值用于嵌入Redis集群的分布式算法中。如果发送节点为从节点，configEpoch是上一次获知的其主节点的configEpoch值。
* 节点标记，标明节点是主是从，以及其他一些单bit的节点信息。
* 发送节点保存的哈希槽。如果是从节点，则表示主节点保存的哈希槽
* 发送节点用于接收客户端请求的端口。
* 发送节点保存的集群状态（down或者ok）
* 主节点ID（如果发送节点是从节点）

ping和pong包还包括了gossip段。该段为接收节点提供了发送节点对其他节点了解的信息。gossip段只含有一些随机节点的信息，不是发送节点知道的所有节点。每个节点信息包含：

* 节点ID
* 节点ip和端口
* 节点标记

通过gossip段，接收节点可以知道发送节点所了解的信息，这有助于故障检测以及节点发现。

### 故障检测

Redis集群故障检测用以识别节点无法被多数节点访问的情况，如果为主节点，则集群会选举一个从节点为新的主节点。如果无法进行选举，集群会进入错误状态并停止接收客户端访问。
上面章节已经提到，每个节点都会维护一个关于其他已知节点的标记列表。其中PFAIL和FAIL两个标记用于故障检测。PFAIL意思是可能有故障，它是一个未经确认的类型。FAIL意思是节点发生了故障并且已在固定时间内被大多数主节点确认。

#### PFAIL标记

当无法访问一个节点超过NODE_TIMEOUT的时间长度时，该节点会被其他节点标记为PFAIL。在Redis集群中“无法访问”的意思是一个激活的ping包（未收到pong响应）持续时间超过NODE\_TIMEOUT。因此NODE\_TIMEOUT需要大于网络包往返的时间。为提高可靠性，当过了NODE\_TIMEOUT一半的时间没有收到pong包，节点会尝试与其他节点重连。因此网络断开通常不会在节点间导致所谓的故障报告。

#### FAIL标记

PFAIL标记是每个节点本地的信息，对于从节点选举的触发来说并不重要。为了让节点被当做故障节点，PFAIL条件需要升级为FAIL条件。PFAIL升级的条件为：

* 假设A节点将B节点标记为PFAIL
* 通过gossip段，A收到多数节点保存的B节点状态。
* 在NODE\_TIMEOUT * FAIL\_REPORT\_VALIDITY\_MULT时间内，多数主节点都将B标记为PFAIL。

满足以上条件后A会将B标记为FAIL，并向所有可访问的节点发送FAIL消息。FAIL消息会强制所有接受到的节点将B标记为FAIL。FAIL标记是单向的，即节点可以从PFAIL变为FAIL，但清除FAIL标记只能在以下场景下发生：

* 节点已经可以访问且为从节点，由于从节点无法被故障转移，故FAIL标记可以被清除。
* 节点已经可以访问且为空的主节点（无哈希槽）。因为空节点不是真正加入了集群，而是在等待配置更新，所以FAIL标记可以被清除。
* 节点已经可以访问且为主节点，经过很长一段时间（N*NODE_TIMEOUT）没有察觉到从节点被选举为主节点，这时最好将该节点重新加入集群使用。

PFAIL升级为FAIL的转变使用了一个脆弱的协议：

* 节点收集其他节点信息会经过一段时间。即使多数节点需要达成一致，但实际上收集到的信息只是表明这些信息是在不同时间和不同节点上收集的，在给定的时刻不能确定，也并不要求多数节点已达成一致。故障报告存在时效性，旧的报告会被节点丢弃。所以多数节点对故障的通知是在一个时间窗口内生效的。
* 虽然节点检测到FAIL条件，就会用FAIL消息将状态强加给其他节点的。但FAIL消息也不确保能够到达所有节点。当网络分区出现时，FAIL消息可能无法通知到其他节点。

然而最终，所有节点都应该对一个给定节点的状态达成一致。如果多数主节点将另外的一个主节点标记为FAIL，其他节点最终也会标记该节点为FAIL，因为在指定的时间窗口内可以收集足够的故障报告。当只有少数节点标记另一个主节点为FAIL时，不会进行从节点选举。根据之前描述的FAIL清除规则，最终所有节点将清除FAIL状态。

FAIL标记仅用于触发从节点选举算法的安全部分，理论上从节点可以独立的启动选取过程，并且如果多数主节点实际上可以访问该主节点，那么从节点只需等待其他主节点拒绝即可。然而PFAIL升级到FAIL状态、状态转变的弱协议和FAIL消息传递机制是有实际意义的。由于以上机制，如果集群进入错误的状态，所有节点通常几乎会在同一时间停止接收请求。使用Redis集群的应用需要这样的特性。

## 配置处理和传播以及故障转移

### 集群当前纪元

Redis集群使用一个与Raft算法中term（可理解为一个逻辑时间）相同的概念。在Redis集群中称为纪元（epoch），用来对事件进行增量式的版本控制。当多个节点提供了冲突的信息时，其他节点可以用纪元来了解哪个状态是最新的。

currentEpoch是一个64位的无符号整数。每个节点在创建时都会将currentEpoch设置为0。每次接收到包时，如果发送者的纪元比本地大，则本地currentEpoch会更新为发送者的纪元。最终所有节点纪元都会达到最大的纪元值。

使用纪元交换的信息用于集群发生变动以及一个节点为执行一些操作而征询同意时。目前只会在从节点选举中使用。本质上纪元对一个集群来讲是一个逻辑时钟。

### 配置纪元

主节点会在ping和pong包中通告自己的configEpoch值以及拥有的哈希槽。在创建新节点时主节点的configEpoch会被设置为0。configEpoch会在slave选举中被置为新值。故障主节点的所有从节点会增加纪元并获取多数主节点的授权。如果一个从节点得到了授权，该从节点会成为主节点并使用一个新创建的configEpoch。
configEpoch有助于解决不同节点声明的、有分歧的配置存在的冲突。

从节点也会在ping和pong包中通告configEpoch，但该值代表的是上一次与其主节点交换包时主节点的configEpoch值。这就使得其他实例可以检测从节点什么时候需要更新配置。configEpoch和currentEpoch都会持久化存储在nodes.conf文件中，在节点继续操作前程序会保证这两个值写入到硬盘中。在故障转移是会确保生成configEpoch是全新的、增量的、且是唯一的。

### 从节点选举和晋升

选举和晋升是由从节点处理的，主节点会帮助进行投票。在有条件成为主节点的从节点中，当至少有一个从节点认为主节点进入了FAIL状态时，就会开始选举流程。处于FAIL状态的主节点的所有从节点都可以发起一次选举，但只有一个从节点会胜出并晋升为主节点。从节点发起选举需要满足的条件为：

* 对应主节点处于FAIL状态
* 主节点拥有哈希槽
* 主从节点间传输副本的连接断开的时间不能超过给定时间（用户可配置），目的是保证晋升的从节点数据足够新的。

为了赢得选举，从节点首先会增加currentEpoch，并向主节点请求投票。从节点通过向每个主节点广播FAILOVER\_AUTH\_REQUEST包发出投票请求。然后等待答复，最多会等待NODE_TIMEOUT两倍的时间（通常至少会等待2秒）。

一旦一个主节点将票投给了一个给定的从节点，会向其回复一个FAILOVER\_AUTH\_ACK包。在NODE_TIMEOUT*2的时间内该主节点不能再给另一个从节点投票。这段期间该主节点也不能回复其他相同故障主节点的从节点发送的授权请求。

如果AUTH_ACK的纪元值小于投票请求发送时从节点的currentEpoch，该节点将丢弃回复包。这样确保了从节点不会统计之前的选举票数。

收到大多数主节点的ACK包的从节点会赢得选举。如果在NODE\_TIMEOUT\*2的时间内没有收到多数回复，当前选举会被中断并在NODE\_TIMEOUT\*4（通常至少4秒 ）时间后再次进行新的选举。

### 从节点排名

主节点一旦处于FAIL状态，从节点在选举前会等待一段时间，这段延迟时间的计算方法为：

<pre>
  DELAY = 500 milliseconds + random delay between 0 and 500 milliseconds +
        SLAVE_RANK * 1000 milliseconds.
</pre>

这段延迟时间保证了FAIL状态在集群中传播，否则从节点可能在尝试被选举，而其他主节点还没有意识到故障的出现，于是拒绝了投票。随机计算的延迟时间用来消除从节点的同步，这样他们就不可能同一时间发起选举。

SLAVE_RANK是关于一个从节点从主节点复制数据量的排名。当主节点故障时，从节点会交换消息来建立排名；副本偏移量更新最多的从节点排名0，第二多的排名1，以此类推。这样一来更新最多的从节点会在其他节点之前发起选举。排名顺序不是严格执行的，如果高排名的从节点选举失败，其他节点会立即尝试选举。

一旦一个从节点赢得选举，它将获得一个新的唯一的configEpoch，这个纪元值比其他主节点都高。之后开始使用ping和pong包通告自己成为了新的主节点，并通知哈希槽集合。

为了加速其他节点的配置更新，pong包会被广播到所有节点。现在当无法访问的节点收到另一个节点的ping或pong包，或者如果他们心跳包发布的信息是过期的，另一个节点会向他们发生UPDATE包。这样他们最终会更新自己的配置。

其他节点检测到一个新的主节点拥有的哈希槽与一个旧的主节点相同，但新节点的configEpoch更高，此时他们会更新自己的配置。旧主节点的从节点不仅会更新配置，还会重新配置为复制新主节点数据，旧主节点重新加入集群时也会如此。

### 主节点对投票请求的响应

主节点收到的投票请求是从节点发送的FAILOVER\_AUTH\_REQUEST请求。以下是允许进行投票的条件：

1. 一个主节点在指定纪元只会投票一次，并会拒绝属于旧纪元的投票：主节点会记录lastVoteEpoch域，只要投票请求中的currentEpoch不大于lastVoteEpoch，主节点就会拒绝投票。当主节点主动回复了投票请求，lastVoteEpoch就会更新并保存在磁盘中。
2. 只有从节点所属的主节点被标记为FAIL，其他主节点才会投票。
3. 如果currentEpoch小于主节点的currentEpoch，投票请求会被主节点忽略。因此主节点回复的currentEpoch通常与投票请求的相同。如果同一从节点重复请求投票，并增加了currentEpoch的值。集群会保证旧的投票回复不会被接收。
4. 主节点在NODE_TIMEOUT*2的时间内不会对同一故障主节点的不同从节点进行重复投票。这条不是严格要求的，因为不太可能两个从节点在同一纪元中都赢得选举。但在实际中，这保证了赢得选举的从节点有足够的时间通知其他从节点，并且避免了另一个节点赢得了一次新的选举，导致了一次不必要的故障转移。
5. 主节点不会在选择最佳从节点上付出任何努力。最佳从节点比其他节点更有可能发起选举并获胜，因为它有更高的排名，通常会更早的开始投票流程。
6. 当主节点拒绝投票时，它不会有任何回复，仅仅是忽略掉请求。
7. 从节点发送的configEpoch比主节点表中该从节点声明的哈希槽对应的任何一个configEpoch小时，没有主节点会为其投票。请求投票的从节点拥有的被转移的哈希槽的配置必须不能旧于给予投票的主节点。

### 网络分区期间，配置纪元用途的实例

本节会说明纪元是怎样使从节点选举过程对网络分区的抵抗力更强。当前场景为：

* 一个主节点将永远无法访问，它有A，B，C三个从节点。
* A赢得了选举并成为新的主节点。
* 一次网络分区使得多数节点无法访问A。
* B赢得选举并成为新的主节点。
* 一次网络分区使得多数节点无法访问B。
* 上一次分区修复，A再次可用。

此时B宕机，A以主节点身份再次可用（实际上UPDATE消息会立即重新配置A，但这里我们假设UPDATE消息全部丢失）。同时C将尝试被推选，此时会发生：

1. C会赢得选举，因为多数主节点认为B（C的主节点）确实无法访问。C会获得一个新的configEpoch。
2. A不能以主节点身份存在，因为其他节点以及将相同的哈希槽关联到更高的配置纪元（B的配置纪元）。
3. 所有节点将更新自己的映射表，将对应的哈希槽赋给C，节点将继续工作。

### 哈希槽配置传播

Redis集群的一个重要机制是传播哪个节点拥有指定哈希槽集合的信息。这对新集群的启动和从节点被推举为新的主节点时更新配置的能力都十分重要。该机制同样允许被分区隔断不确定时间的节点以合理的方式重新加入集群。

哈希槽配置传播有两种方法：

1. 心跳包。ping或pong包会加入其拥有的（或其主节点拥有的）哈希槽信息
2. UPDATE消息。由于每个心跳包都会包含发送者的configEpoch和拥有的哈希槽，因此如果接收者发现发送者的信息过期，接收者会使用新配置给发送者发送包，强迫其更新信息。

心跳包或UPDATE消息的接收者使用非常简单的机制更新映射表。新节点创建时，它的映射表的每个入口会简单的初始化成NULL，这样每个哈希槽都是未绑定的。类似于：

<pre>
0 -> NULL
1 -> NULL
2 -> NULL
...
16383 -> NULL
</pre>

节点更新映射表的第一条规则为：
如果一个哈希槽为赋值，且一个已知节点声明了所有权，我将修改映射表来关联该哈希槽和该节点。

如果节点A声明了哈希槽1和2的所有权，其配置纪元值为3，那么映射表将修改为：

<pre>
0 -> NULL
1 -> A [3]
2 -> A [3]
...
16383 -> NULL
</pre>

然而一条规则还不够，我们知道哈希槽映射在两种情况下会改变：

1. 从节点在故障转移期间替换了主节点
2. 哈希槽重新分配给其他节点

现在先关注在故障转移的情况。当从节点完成故障转移，它将获得新的配置纪元值，且比之前的主节点的大。之后它开始发送心跳包。由于下面的第二条规则，接收者会更新映射表：
某一哈希槽已分配，一个已知节点告知自己拥有该哈希槽，且它使用的configEpoch比映射表中对应的当前主节点的值要大。此时我会将该槽重新绑定到新节点。

因此当从B节点收到了声明哈希槽1和2的消息，且消息使用的配置纪元值为4时，接收者将更新映射表为：

<pre>
0 -> NULL
1 -> B [4]
2 -> B [4]
...
16383 -> NULL
</pre>

由于第二条规则，所有节点最终会达成一致：在所有发送通告的节点中，一个哈希槽的所有者是拥有最大configEpoch值的节点。

这个机制被称为最近一次故障转移生效（last failover wins）。重新分片的机制是一样的，当一个节点完成导入操作后，它的配置纪元值会增加从而保证变更在节点中传播。

### UPDATE消息

在上一节的示例中，A节点可能会重新加入集群，其发送的心跳包会声明自己拥有哈希槽1和2，但配置纪元值为3。所有接收者会发现对应的哈希槽的所有者是B，且配置纪元为4。因此接收者会使用新配置向A发送UPDATE消息。A基于规则2来更新自己的配置。

### 节点如何重新加入集群

接着之前的例子，假设A只有1和2两个哈希槽，A拥有的哈希槽数量将变为0。之后A会重新配置成从节点。实际的规则会更复杂一些。通常A会经过很长一段时间才重新加入集群，此时A之前拥有的哈希槽可能已经被分配给多个节点，例如B拥有槽1，C拥有槽2。

因此实际的规则是：主节点会修改配置并成为副本，它的新主节点是拥有它最后一个哈希槽的节点。不仅是主节点，从节点也会遵循相同规则：从节点会重新配置为副本，其新主节点是拥有旧主节点最后一个哈希槽的节点。

### configEpoch冲突解决算法

通过从节点选举生成的configEpoch可以保证唯一性。但有两个事件会使得configEpoch使用不安全的方式生成：仅使用本地currentEpoch加1作为新的configEpoch，并寄希望于不会存在冲突。这两个事件均为系统管理员触发的事件：

1. 带TAKEOVER参数的CLUSTER FAILOVER指令，该指令推选一个从节点成为主节点，不管多数主节点是否可用。在多数据中心安装时会用到。
2. 哈希槽迁移

当一个哈希槽从A移向B，程序会强迫B将配置更新到纪元最大的配置，再加1（除非该节点配置纪元就是最大值），不管其他节点是否同意。现实中重新分片通常都会涉及几百个哈希槽。要求每个哈希槽迁移后都协商一个新的配置纪元是低效的。
此外保存新配置会使用fsync操作，因此只需要在第一个哈希槽移动时生成新配置纪元即可，这样在生产环境就更加高效了。

由于上述两种情况，有可能（但可能性不大）最终多个节点拥有相同的配置纪元。系统管理员执行了重新分片的操作，同时发生了一次故障转移，此时currentEpoch如果传播不够快的话可能会出现冲突。

Redis集群的主要的活性属性要求哈希槽配置要收敛，所以在任何情况下我们都想让所有主节点拥有不同的configEpoch。为此Redis集群实现了一个冲突解决算法：

* 如果一个主节点发现另一个主节点使用相同的configEpoch通告自己的信息时。
* 且如果节点的ID在字典顺序上小于拥有相同configEpoch值的主节点
* 那么该节点用currentEpoch值加1作为新的configEpoch。

如果存在任意大小集合的节点使用相同的configEpoch，集合中除了ID最大的节点外所有节点的configEpoch值都会增加，保证最终每个节点拥有的configEpoch是唯一的。

### 节点重置

为了以不同角色或不同集群中重用节点，可以将节点进行重置（不用重启）。重置使用CLUSTER RESET指令，该指令提供两个参数：

* CLUSTER RESET SOFT
* CLUSTER RESET HARD

该指令必须在被重置的节点上执行。如果不指定参数，默认使用SOFT参数。重置指令将执行以下操作：

1. 如果节点是从节点，它将转变成主节点且丢弃全部数据。如果节点是主节点且拥有主键，重置操作将中断。
2. 所有哈希槽将被释放，手动故障转移状态会被复位。
3. 映射表中的其他节点会被移除，该节点将不再知道其他节点的存在
4. 如果为HARD重置，currentEpoch、configEpoch和lastVoteEpoch将置为0.
5. 如果为HARD重置，节点ID将重新生成。

非空主节点无法被重置。然而在一些适合重置主节点的特殊场景下（如为了创建新集群而完全销毁当前集群时），须在重置前执行FLUSHALL指令。

### 移除节点

实际中可能会需要通过迁出所有数据并关闭的方式在集群中移除一个节点。然而其他节点依然会记录该节点的ID和地址，并试图与其建立连接。为此当移除节点时，我们想要同时移除其他节点映射表中该节点入口。此时需要执行CLUSTER FORGET node-id指令，该指令会做两件事：

1. 在节点表中移除指定ID的节点。
2. 在60秒内，禁止相同ID的节点重新加入。

执行第二条操作是有原因的，Redis集群使用gossip协议来自动发现节点。因此从节点A的表中移除X，节点B可能会将X节点信息再次通知给A节点。因为60秒禁令的存在，系统管理工具可以有60秒的时间将X从所有节点的表中移除，并阻止自动发现功能将X重新加入到集群。

## Redis集群配置

<pre>
    * cluster-enabled <yes/no>：集群开关
    * cluster-config-file <filename>：此项不是用于用户编辑的配置文件，而是指Redis集群自动保存的一些配置信息，以便重启时可以再次获取这些配置。该文件会记录如集群中其他的节点、各节点状态、一些变量等等。默认为nodes.conf
    * cluster-node-timeout <milliseconds>：节点超时
    * cluster-slave-validity-factor <factor>：如果设置为0，不管与主节点连接断开多久，从节点都会尝试故障恢复。如果设置为正整数，该配置会与节点超时的值相乘，得到一个最大连接断开时间，只有超过最大连接断开时间，从节点才会尝试故障恢复。需要注意的是，如果没有从节点可以完成故障恢复，任何非0值都可能导致集群不可用。在这种情况下，只有原始的主节点重新加入集群才能是集群恢复可用性。
    * cluster-migration-barrier <count>：一个主节点需要保留的从节点数量的最小值，其余从节点可以迁移给没有从节点的主节点。
    * cluster-require-full-coverage <yes/no>：如果为no，集群可以继续支持查询即使只有一部分主键可以被访问。默认为yes。
</pre>

## Redis集群的创建和使用

创建集群需要使用redis-trib.rb脚本，需要安装ruby并下载gem文件：
https://rubygems.global.ssl.fastly.net/gems/redis-3.0.5.gem
下载完成后执行gem install -l redis-3.0.5.gem

### 创建集群

可以使用源码目录下的utils/create-cluster/create-cluster脚本创建集群，该脚本是shell脚本，可以根据情况修改。
创建过程：

* 进入utils/create-cluster/目录
* 执行./create-cluster start
* 执行./create-cluster create

该脚本默认会创建三主三从6个节点，占用30001端口到30006端口。使用create-cluster stop可以停止服务。

使用redis-cli -c -p 30001可以访问集群

### 重新分片

使用以下指令可以执行分片操作：

<pre>
    ./redis-trib.rb reshard 127.0.0.1:30001
</pre>

执行后会出现的第一个提问是需要移动的哈希槽个数：

<pre>
    How many slots do you want to move (from 1 to 16384)?
</pre>

第二个问题是移动的目的地：

<pre>
    What is the receiving node ID?
</pre>

此时需要获取目的节点的ID，可通过以下指令查询：

<pre>
$ redis-cli -p 30001 cluster nodes |grep myself
4602cbb9e89b62c5cc3ea19b8202c7d90c6966fc 127.0.0.1:30001 myself,master - 0 0 7 connected 0-5460
</pre>

其中第一个值4602cbb9e89b62c5cc3ea19b8202c7d90c6966fc就是ID

CLUSTER NODES指令输出的信息为：

<pre>
    * 节点ID
    * ip:port
    * 标记：master, slave, myself, fail, ...
    * 如果是从节点，显示主节点ID
    * 上一个PING等待回复的时间
    * 上一个PONG接收的时间
    * epoch配置
    * 节点连接状态
    * 包含的哈希槽
</pre>

第三个问题是源节点ID，输入all会选择所有节点移动。如果需要指定节点，则一次输入一个ID，最后输入done表示完成。

第四个问题是分片计划的确认，输入yes后可看到哈希槽移动的日志。

使用以下指令可以检查重新分片后的集群健康情况：

<pre>
    redis-trib.rb check 127.0.0.1:30001
</pre>

使用以下指令可以通过参数直接执行分片，而避免交互模式参数输入：

<pre>
    ./redis-trib.rb reshard --from <node-id> --to <node-id> --slots <number of slots> --yes <host>:<port>
</pre>

手动重新分片过程中，多主键操作可能在一段时间内无法使用，而单主键操作总是可以执行。

### 故障恢复测试

可以在主节点执行DEBUG SEGFAULT指令使其宕机

<pre>
# redis-cli -p 30002 debug segfault
Error: Server closed the connection
</pre>

查看当前集群的状态

<pre>
# redis-cli -p 30001 cluster nodes

4602cbb9e89b62c5cc3ea19b8202c7d90c6966fc 127.0.0.1:30001 myself,master - 0 0 7 connected 0-5961 10923-11421
c02d3348e271ae46e54e3652898affc2e7e381be 127.0.0.1:30005 master - 0 1449639614399 8 connected 5962-10922
88db6a5470e59db09567d1b2cd48693df4636838 127.0.0.1:30003 master - 0 1449639614400 3 connected 11422-16383
29cce06d16df379d9603e5933d173b7f1d20bfd2 127.0.0.1:30006 slave 88db6a5470e59db09567d1b2cd48693df4636838 0 1449639614399 6 connected
3105795d9ab7c6e349d26522afbe8af621056b82 127.0.0.1:30004 slave 4602cbb9e89b62c5cc3ea19b8202c7d90c6966fc 0 1449639614399 7 connected
d4a2d4d7d4125f2cadc1939905a2cbfcd04545aa 127.0.0.1:30002 master,fail - 1449639376156 1449639375151 2 disconnected
</pre>

可以发现30005自动被推选为新的master，30002是一个失效的master并处于disconnected状态。

重启30002并再查看集群状态：

<pre>
# redis-server --port 30002 --cluster-enabled yes --cluster-config-file nodes-30002.conf --cluster-node-timeout 2000 --appendonly yes --appendfilename appendonly-30002.aof --dbfilename dump-30002.rdb --logfile 30002.log --daemonize yes

# redis-cli -p 30001 cluster nodes         

4602cbb9e89b62c5cc3ea19b8202c7d90c6966fc 127.0.0.1:30001 myself,master - 0 0 7 connected 0-5961 10923-11421
c02d3348e271ae46e54e3652898affc2e7e381be 127.0.0.1:30005 master - 0 1449639739032 8 connected 5962-10922
88db6a5470e59db09567d1b2cd48693df4636838 127.0.0.1:30003 master - 0 1449639739032 3 connected 11422-16383
29cce06d16df379d9603e5933d173b7f1d20bfd2 127.0.0.1:30006 slave 88db6a5470e59db09567d1b2cd48693df4636838 0 1449639739031 6 connected
3105795d9ab7c6e349d26522afbe8af621056b82 127.0.0.1:30004 slave 4602cbb9e89b62c5cc3ea19b8202c7d90c6966fc 0 1449639739031 7 connected
d4a2d4d7d4125f2cadc1939905a2cbfcd04545aa 127.0.0.1:30002 slave c02d3348e271ae46e54e3652898affc2e7e381be 0 1449639739232 8 connected
</pre>

可以看到30002重启后变成了30005的从节点。

### 手动触发故障恢复

有时在没有主节点出现故障时手动触发故障恢复是很有用的，例如需要升级主节点版本时，手动触发故障恢复可以将对可用性的影响降到最低。
CLUSTER FAILOVER指令用来触发故障恢复，该指令必须在故障转移的主节点的其中一个从节点上执行。
手动触发故障恢复会比真正出现主节点故障要安全。只有当系统确认新的master已经处理所有复制工作之后，客户端访问才会被切换到新的master，通过这样的机制可以避免数据丢失。
基本原理是：连接到正在故障转移的master节点的客户端访问会被阻塞。此时master会向slave节点发送副本偏移量，slave等待本地的复制进度达到偏移量。当两边的偏移量相同后便开始故障恢复，配置变更会通知到旧master节点。然后之前阻塞的客户端请求会被重定向到新master节点。

### 添加节点

#### 添加主节点

无论是添加master节点还是slave节点，首先都需要创建一个空节点。

创建一个Redis配置文件 redis-30007.conf并添加以下内容：

<pre>
port 30007

cluster-enabled yes

cluster-config-file nodes-30007.conf

cluster-node-timeout 2000

appendonly yes

appendfilename appendonly-30007.aof

dbfilename dump-30007.rdb

logfile 30007.log

daemonize yes
</pre>

使用该配置启动新节点

<pre>
    redis-server redis-30007.conf
</pre>

使用以下指令将新节点添加到集群中：

<pre>
# redis-trib.rb add-node 127.0.0.1:30007 127.0.0.1:30001

>>> Adding node 127.0.0.1:30007 to cluster 127.0.0.1:30001
Connecting to node 127.0.0.1:30001: OK
Connecting to node 127.0.0.1:30005: OK
Connecting to node 127.0.0.1:30003: OK
Connecting to node 127.0.0.1:30006: OK
Connecting to node 127.0.0.1:30004: OK
Connecting to node 127.0.0.1:30002: OK
>>> Performing Cluster Check (using node 127.0.0.1:30001)
M: 4602cbb9e89b62c5cc3ea19b8202c7d90c6966fc 127.0.0.1:30001
   slots:0-5961,10923-11421 (6461 slots) master
   1 additional replica(s)
M: c02d3348e271ae46e54e3652898affc2e7e381be 127.0.0.1:30005
   slots:5962-10922 (4961 slots) master
   1 additional replica(s)
M: 88db6a5470e59db09567d1b2cd48693df4636838 127.0.0.1:30003
   slots:11422-16383 (4962 slots) master
   1 additional replica(s)
S: 29cce06d16df379d9603e5933d173b7f1d20bfd2 127.0.0.1:30006
   slots: (0 slots) slave
   replicates 88db6a5470e59db09567d1b2cd48693df4636838
S: 3105795d9ab7c6e349d26522afbe8af621056b82 127.0.0.1:30004
   slots: (0 slots) slave
   replicates 4602cbb9e89b62c5cc3ea19b8202c7d90c6966fc
S: d4a2d4d7d4125f2cadc1939905a2cbfcd04545aa 127.0.0.1:30002
   slots: (0 slots) slave
   replicates c02d3348e271ae46e54e3652898affc2e7e381be
[OK] All nodes agree about slots configuration.
>>> Check for open slots...
>>> Check slots coverage...
[OK] All 16384 slots covered.
Connecting to node 127.0.0.1:30007: OK
>>> Send CLUSTER MEET to node 127.0.0.1:30007 to make it join the cluster.
[OK] New node added correctly.
</pre>

使用以下指令查看集群信息，检查节点是否添加成功：

<pre>
# redis-cli -p 30007 cluster nodes

29cce06d16df379d9603e5933d173b7f1d20bfd2 127.0.0.1:30006 slave 88db6a5470e59db09567d1b2cd48693df4636838 0 1449703240357 3 connected
4602cbb9e89b62c5cc3ea19b8202c7d90c6966fc 127.0.0.1:30001 master - 0 1449703240357 7 connected 0-5961 10923-11421
3105795d9ab7c6e349d26522afbe8af621056b82 127.0.0.1:30004 slave 4602cbb9e89b62c5cc3ea19b8202c7d90c6966fc 0 1449703240959 7 connected
d4a2d4d7d4125f2cadc1939905a2cbfcd04545aa 127.0.0.1:30002 slave c02d3348e271ae46e54e3652898affc2e7e381be 0 1449703240357 8 connected
c02d3348e271ae46e54e3652898affc2e7e381be 127.0.0.1:30005 master - 0 1449703240758 8 connected 5962-10922
23cf90cadf132058a360e2b65af0a61aebbbaa11 127.0.0.1:30007 myself,master - 0 0 0 connected
88db6a5470e59db09567d1b2cd48693df4636838 127.0.0.1:30003 master - 0 1449703240357 3 connected 11422-16383
</pre>

与其他master相比，新增的空节点有两个特点：

* 因为没有哈希槽，所以不会存储任何数据。
* 故障恢复不会生效

现在可以通过之前的示例向该节点分配哈希槽了。

<pre>
    redis-trib.rb reshard --from 4602cbb9e89b62c5cc3ea19b8202c7d90c6966fc --to 23cf90cadf132058a360e2b65af0a61aebbbaa11 --slots 1000 --yes 127.0.0.1:30001
</pre>

#### 添加从节点

用同样方法创建一个使用30008端口的空节点。然后执行以下指令：

<pre>
#  redis-trib.rb add-node --slave 127.0.0.1:30008 127.0.0.1:30001

>>> Adding node 127.0.0.1:30008 to cluster 127.0.0.1:30001
Connecting to node 127.0.0.1:30001: OK
Connecting to node 127.0.0.1:30005: OK
Connecting to node 127.0.0.1:30003: OK
Connecting to node 127.0.0.1:30006: OK
Connecting to node 127.0.0.1:30008: OK
Connecting to node 127.0.0.1:30004: OK
Connecting to node 127.0.0.1:30002: OK
Connecting to node 127.0.0.1:30007: OK
>>> Performing Cluster Check (using node 127.0.0.1:30001)
M: 4602cbb9e89b62c5cc3ea19b8202c7d90c6966fc 127.0.0.1:30001
   slots:1000-5961,10923-11421 (5461 slots) master
   1 additional replica(s)
M: c02d3348e271ae46e54e3652898affc2e7e381be 127.0.0.1:30005
   slots:5962-10922 (4961 slots) master
   1 additional replica(s)
M: 88db6a5470e59db09567d1b2cd48693df4636838 127.0.0.1:30003
   slots:11422-16383 (4962 slots) master
   1 additional replica(s)
S: 29cce06d16df379d9603e5933d173b7f1d20bfd2 127.0.0.1:30006
   slots: (0 slots) slave
   replicates 88db6a5470e59db09567d1b2cd48693df4636838
S: 0a78f6b6f3e5f4d79e7975e00f433fd2add1d04d 127.0.0.1:30008
   slots: (0 slots) slave
   replicates 23cf90cadf132058a360e2b65af0a61aebbbaa11
S: 3105795d9ab7c6e349d26522afbe8af621056b82 127.0.0.1:30004
   slots: (0 slots) slave
   replicates 4602cbb9e89b62c5cc3ea19b8202c7d90c6966fc
S: d4a2d4d7d4125f2cadc1939905a2cbfcd04545aa 127.0.0.1:30002
   slots: (0 slots) slave
   replicates c02d3348e271ae46e54e3652898affc2e7e381be
M: 23cf90cadf132058a360e2b65af0a61aebbbaa11 127.0.0.1:30007
   slots:0-999 (1000 slots) master
   1 additional replica(s)
[OK] All nodes agree about slots configuration.
>>> Check for open slots...
>>> Check slots coverage...
[OK] All 16384 slots covered.
Automatically selected master 127.0.0.1:30001
Connecting to node 127.0.0.1:30008: OK
[ERR] Node 127.0.0.1:30008 is not empty. Either the node already knows other nodes (check with CLUSTER NODES) or contains some key in database 0.
</pre>

与添加master不同，添加从节点的指令会使用--slave选项。redis-trib脚本会将新增节点随机添加给从节点较少的master节点。

此外，添加节点时还可以指定目标master，指令如下：

<pre>
    redis-trib.rb add-node --slave --master-id 3c3a0c74aae0b56170ccb03a76b60cfe7dc1912e 127.0.0.1:7006 127.0.0.1:7000
</pre>

首先添加一个空master节点，再使用CLUSTER REPLICATE指令将其转为从节点是另一种手动指定master的方法，此方法还可以用来将一个已添加的slave节点转成其他master的从节点。该指令需要在被转移的从节点上执行。

### 删除节点

通过以下指令可以删除节点：

<pre>
    redis-trib.rb del-node 127.0.0.1:30001 `<node-id>`
</pre>

第一个参数是集群中任意的节点，第二个参数是被删除的节点ID。
从节点可以被直接删除，但在删除主节点前，必须保证主节点是一个空节点。因此，需要将待删除的master节点的哈希槽全部分配给其他master节点。

### 副本迁移（Replicas migration）

使用CLUSTER REPLICATE指令可以将从节点重新分配给其他主节点。Redis集群支持一种叫做“副本迁移”的功能，该功能可以自动分配从节点从而提高系统可靠性。
Redis集群对故障的抵抗能力是与属于给定主节点的从节点数量相关的，这也是需要副本迁移的原因。例如，在一主一从的集群中，主节点和对应从节点同时故障后，这个集群将无法正常服务。或许如网络断裂、软硬件故障等故障很难发生，但也很有可能从节点在4点出了故障而没有被发现，而主节点在6点出现故障，此时集群将无法正常工作。
为了提高可靠性，可以为每个主节点多加一个从节点，但这样的代价是比较昂贵的。如集群中有10个主节点，分别对应一个从节点，集群由20个节点组成，如果每个主节点再增加1个从节点，那么整个集群的节点数量将达到30个。基于副本迁移，只需要为部分主节点添加从节点即可提高可靠性。
当一个master没有从节点时，其他拥有多个从节点的主节点的一个副本会被迁移给这个“被孤立”的master。在之前的示例中，如果从节点在4点出现了故障，其他master的从节点会取而代之。当主节点6点出现故障时，被迁移的从节点仍可以提供服务。

简而言之，副本迁移可以：

* 集群会选择拥有最多从节点的主节点的一个副本进行迁移
* 得益于副本迁移，我们只需给一部分任意的主节点添加从节点，而不是全部。
* cluster-migration-barrier配置会影响副本迁移，该配置会保证从节点数量少于指定值时，该master节点的从节点不会参与副本迁移。 

#### 副本迁移算法

副本迁移算法不会有任何协商，因为从节点结构不是集群配置中需要同步或使用纪元描述的部分。迁移算法是用来避免大量的从节点迁移，算法会保证最终每个主节点都会拥有至少一个从节点。

在解释算法之前，首先需要定义一个健康从节点的概念：健康从节点对于一个给定的节点来说就是不处在FAIL状态的从节点。

每个检测到至少一个主节点没有健康从节点的从节点都可以触发迁移算法。然而这些从节点中只会有一个子集执行算法，通常子集只有单个从节点，除非不同的从节点在指定的时间检测到其他节点的故障状态稍有不同。

执行算法的从节点属于从节点数量最多的主节点，该从节点不在FAIL状态且有最小的节点ID。例如集群中有10个主节点拥有1个从节点，2个主节点拥有5个从节点。尝试迁移的节点是属于2个拥有5个从节点的主节点，且节点ID最小的从节点。迁移操作不做任何协商，所以有可能在集群配置不稳定时，许多从节点会认为自己是执行迁移的节点（实际中不可能出现）从而产生了竞争。如果发生上述情况，结果就是这些节点迁移到同一个主节点，完全无害。如果此时节点迁出使得一个主节点没有任何从节点，只要集群配置恢复稳定，迁移算法会再次执行，一个从节点会被迁回给该主节点。最终每个主节点都会只是拥有一个从节点。

迁移操作通常都是在拥有多个从节点的主节点的从节点中选择一个迁移到成为“孤儿”的主节点。迁移算法受cluster-migration-barrier配置项控制。它配置了在迁出从节点之前，主节点必须保留的健康从节点数量。例如配置为2，只有当主节点拥有2两个以上工作的从节点时才会迁出从节点。

### 升级节点

升级从节点非常简单，只需停止节点然后更新Redis版本再重启即可。如果客户端可以在从节点中读取数据，这些客户端需要能够重连到其他从节点。

升级主节点比较复杂一点，建议按以下步骤执行：

1. 使用CLUSTER FAILOVER指令触发手动故障恢复（参考之前的章节）
2. 等待主节点变成从节点
3. 升级该节点版本
4. 如果希望升级的节点变成主节点，再次手动触发故障恢复。

### 迁移到Redis集群

希望迁移到Redis集群用户可能只使用了一个主节点，或者已经通过客户端程序或Redis代理使用了一个分片算法。
不管是何种情况，迁移到Redis集群都很简单。最重要的细节是是否在应用中使用了多个主键的操作以及怎样操作的。以下是三种不同情况：

1. 没有使用多主键操作、事务或包含多主键操作的Lua脚本。主键都是被独立访问的（即使通过事务或Lua脚本将关于相同主键的多个指令组合在一起）
2. 使用了多主键操作、事务或使用多主键的Lua脚本，但主键都有相同的哈希标签。
3. 多主键操作、事务或使用多主键的Lua脚本，这些主键没有明确的或相同的名称、哈希标签

Redis集群无法处理第三种情况，这样的应用需要进行修改以便不使用多主键操作或使用相同的哈希标签。
前两种情况处理方式一样。假设数据已拆分到N个主节点，执行以下步骤以迁移到Redis集群：

1. 停止客户端访问。目前Redis集群没有自动的在线迁移。
2. 使用BGREWRITEAOF指令为所有N个主节点生成AOF文件，并等待AOF文件生成完成。
3. 保存这些AOF文件。此时如果需要，你可以停止旧节点的服务。
4. 创建一个含有N个主节点且没有从节点的集群。确保所有节点都在使用AOF持久化功能。
5. 停止所有节点，使用之前保存的AOF文件逐个替换集群节点的AOF文件。
6. 使用替换后的AOF文件重启集群。通过配置，集群中的节点会发现并警告存在不该存在的主键。
7. 使用redis-trib fix指令修复集群，以便主键会被迁移到集群中。
8. 最后使用redis-trib check，确保集群没问题
9. 使用支持Redis集群的客户端访问服务。

使用redis-trib import指令是另一个可以将数据导入到集群中的方法。这个指令会将主键从指定的节点移动到集群中。由于2.8版本没有实现迁移连接如果用2.8版本的Redis节点作为源示例，这个操作会很慢。
